{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 2. Exercise 2. }$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it\n",
        "  if len(x) == 3:\n",
        "    return (1/8)*((x[0]-2)**2)+(1/(8**2))*((x[1]-4)**2) + (1/(8**3))*((x[2]-8)**2)\n",
        "  if len(x) == 4:\n",
        "    return (1/8)*((x[0]-2)**2)+(1/(8**2))*((x[1]-4)**2) + (1/(8**3))*((x[2]-8)**2) + (1/(8**4))*((x[3]-16)**2)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  #print (len(x))\n",
        "  #print (\"a\", np.array([(1/4)*(x[0]-2),(1/32)*(x[1]-4),(1/256)*(x[2]-8),(1/2048)*(x[3]-16)])\n",
        "  if len(x) == 3:\n",
        "    return np.array([(1/4)*(x[0]-2),(1/32)*(x[1]-4),(1/256)*(x[2]-8)])\n",
        "  if len(x) == 4:\n",
        "    return np.array([(1/4)*(x[0]-2),(1/32)*(x[1]-4),(1/256)*(x[2]-8),(1/2048)*(x[3]-16)])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_exact(gradf, A): #add appropriate arguments to the function \n",
        "  assert type(gradf) is np.ndarray #and len(gradf) == 2 \n",
        "  assert type(A) is np.ndarray #and A.shape[0] == 2 and  A.shape[1] == 2 #allow only a 2x2 array\n",
        "   \n",
        "  #Complete the code to compute step length\n",
        "  #print(gradf)\n",
        "  #print(A)\n",
        "  nume=np.matmul(gradf.transpose(),gradf)\n",
        "  deno=np.matmul(np.matmul(gradf.transpose(),A),gradf)\n",
        "  step_length=nume/(2*deno)  \n",
        "  return step_length"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "source": [
        "#Complete the module to compute the steplength by using the backtracking line search\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray #and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray #and len(gradf) == 2 \n",
        "  \n",
        "  alpha = alpha_start\n",
        "  pk=-gradf\n",
        "  #implement the backtracking line search\n",
        "  while evalf(x+alpha*pk)>evalf(x)+gamma*alpha*np.matmul(gradf.T,pk):\n",
        "    alpha=rho*alpha\n",
        "\n",
        "  #print('final step length:',alpha)\n",
        "  return alpha"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaUUdzLtVSCl"
      },
      "source": [
        "#we define the types of line search methods that we have implemented\n",
        "EXACT_LINE_SEARCH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "CONSTANT_STEP_LENGTH = 3"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol, line_search_type, *args):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray #and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  # construct a suitable A matrix for the quadratic function \n",
        "  if len(start_x) == 3:\n",
        "    A = np.array([[1/8, 0, 0],[0, 1/64, 0],[0, 0, 1/512]])\n",
        "  if len(start_x) == 4:\n",
        "    A = np.array([[1/8, 0, 0, 0], [0, 1/64, 0, 0], [0, 0, 1/512, 0], [0, 0, 0, 1/4096]])\n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "\n",
        "  #initialization for backtracking line search\n",
        "  if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "    #print('Params for Backtracking LS: alpha start:', alpha_start, 'rho:', rho,' gamma:', gamma)\n",
        "\n",
        "  k = 0\n",
        "  #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "  \n",
        "    if line_search_type == EXACT_LINE_SEARCH:\n",
        "      #print(g_x)\n",
        "      step_length = compute_steplength_exact(g_x, A) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('EXACT LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "      step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) #call the new function you wrote to compute the steplength\n",
        "      #raise ValueError('BACKTRACKING LINE SEARCH NOT YET IMPLEMENTED')\n",
        "    elif line_search_type == CONSTANT_STEP_LENGTH: #do a gradient descent with constant step length\n",
        "      step_length = 0.1\n",
        "    else:  \n",
        "      raise ValueError('Line search type unknown. Please check!')\n",
        "    \n",
        "    #implement the gradient descent steps here   \n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "\n",
        "    #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return x, k, evalf(x)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 2"
      ],
      "metadata": {
        "id": "9WDk8aZMP02E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([(1/64),(1/8),1])\n",
        "my_tol=10**(-10)"
      ],
      "metadata": {
        "id": "NDo02rqNP1yD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_const, iter_const, f_const = find_minimizer(my_start_x, my_tol, CONSTANT_STEP_LENGTH)\n",
        "print(\"Minimizer =\", x_opt_const)\n",
        "print(\"Minimum function value =\", f_const)\n",
        "print(\"No of iterations =\", iter_const)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02a1IYwjP1WG",
        "outputId": "64366102-b8de-4422-8e76-95cd3b0b17f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [2.         4.         7.99999997]\n",
            "Minimum function value = 1.2793410906926164e-18\n",
            "No of iterations = 49723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 3"
      ],
      "metadata": {
        "id": "b53YC_MxeC5A"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4"
      },
      "source": [
        "my_start_x=np.array([(1/64),(1/8),1])\n",
        "my_tol=10**(-10)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_exact, iter_exact, f_exact = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(\"Minimizer =\", x_opt_exact)\n",
        "print(\"Minimum function value =\", f_exact)\n",
        "print(\"No of iterations =\", iter_exact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr794bWOcsDf",
        "outputId": "8491eba6-d836-4b39-8697-0d4d1150d137"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [2.         4.         7.99999998]\n",
            "Minimum function value = 9.150071377581033e-19\n",
            "No of iterations = 269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "x_opt_bls, iter_bls, f_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(\"Minimizer =\", x_opt_bls)\n",
        "print(\"Minimum function value =\", f_bls)\n",
        "print(\"No of iterations =\", iter_bls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzjvxh3Bbt-3",
        "outputId": "5549037b-7138-4778-d0af-1660dfd4e460"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [2.         4.         7.99999997]\n",
            "Minimum function value = 1.2748574165464873e-18\n",
            "No of iterations = 4964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 4"
      ],
      "metadata": {
        "id": "t9U6ZbbleFzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x=np.array([(1/512),(1/64),(1/8),1])\n",
        "my_tol=10**(-10)"
      ],
      "metadata": {
        "id": "ryoemgwCnK4b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_opt_exact, iter_exact, f_exact = find_minimizer(my_start_x, my_tol, EXACT_LINE_SEARCH)\n",
        "print(\"Minimizer =\", x_opt_exact)\n",
        "print(\"Minimum function value =\", f_exact)\n",
        "print(\"No of iterations =\", iter_exact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQTspFvRnMsD",
        "outputId": "e4b1d835-6bd4-4072-a7fd-5992ef255e14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [ 2.          4.          8.         15.99999981]\n",
            "Minimum function value = 8.8565993523583e-18\n",
            "No of iterations = 2013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check what happens when you call find_minimzer using backtracking line search\n",
        "x_opt_bls, iter_bls, f_bls = find_minimizer(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, 1, 0.5,0.5)\n",
        "print(\"Minimizer =\", x_opt_bls)\n",
        "print(\"Minimum function value =\", f_bls)\n",
        "print(\"No of iterations =\", iter_bls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TZBz9Y0nOQ2",
        "outputId": "bb6e51e6-9e59-49dc-8811-c63852ab7fa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [ 2.         4.         8.        15.9999998]\n",
            "Minimum function value = 1.0237544252113035e-17\n",
            "No of iterations = 37079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 2) Minimizer = [2,4,8] and minimum function value = 0"
      ],
      "metadata": {
        "id": "6rjrppm9JEbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 3) Backtracking line search procedure takes 4964 iterations whereas exact line search procedure takes 269 iterations. "
      ],
      "metadata": {
        "id": "dhosCRsYJEQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 4) Minimizer = [2,4,8,16] and minimum function value = 0\n",
        "\n",
        "Backtracking line search procedure takes 37079 iterations whereas exact line search procedure takes 2013 iterations. "
      ],
      "metadata": {
        "id": "Z3FaV0rzJEMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 5) For any N, Minimizer = [ 2, 4, 8, 16, ... , $2^N$ ] where $i^{th}$ coordinate = $2^i$ and minimum function value = 0.\n",
        "\n",
        "Usually, backtracking line search takes more iterations than exact line search because of the parameters we specify (alpha, rho and gamma). \n",
        "\n"
      ],
      "metadata": {
        "id": "kWo7SokMJEFw"
      }
    }
  ]
}