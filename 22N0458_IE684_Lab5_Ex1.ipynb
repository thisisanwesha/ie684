{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 5.} \\large\\textbf{Exercise 1.}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "Recall that to solve problems of the form $\\min_{\\mathbf{x} \\in {\\mathbb{R}}^n} f(\\mathbf{x})$, the update rule involved in Newton's method is of the form: \n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - \\eta^k (\\nabla^2 f(\\mathbf{x}^{k}))^{-1} \\nabla f(\\mathbf{x}^{k}).   \n",
        "\\end{align}\n",
        "\n",
        "Now we will discuss a method which avoids explicit computation of the inverse of Hessian matrix at each iteration, but is nearly efficient as the Newton's method. This method will be called BFGS named after the famous applied Mathematicians Broyden, Fletcher, Goldfarb and Shanno. \n",
        "\n",
        "The main idea of BFGS method is to replace the inverse of Hessian matrix $(\\nabla^2 f(\\mathbf{x}^{k}))^{-1}$ in the update rule of Newton's method with a surrogate term $B^k$. \n",
        "\n",
        "Therefore the update rule of BFGS looks as follows:\n",
        "\\begin{align}\n",
        "\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - \\eta^k B^k \\nabla f(\\mathbf{x}^{k})   \n",
        "\\end{align}\n",
        "where $B^k$ is a surrogate for the inverse of Hessian matrix. \n",
        "\n",
        "To find a suitable candidate for $B^k$, we need to consider some favorable characteristics expected from $B^k$: \n",
        "\n",
        "\\begin{align}\n",
        "&B^k \\text{ is symmetric positive definite}.  \\\\\n",
        "&B^k \\text{ does not involve computing Hessian or its inverse and should be computable only from the gradients}.  \\\\\n",
        "&\\text{Replacing  } (\\nabla^2 f(\\mathbf{x}^{k}))^{-1} \\text{ with } B^k \\text{ should not slow down the algorithm too much}. \\\\ \n",
        "\\end{align}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L9tVzAnqyb-"
      },
      "source": [
        "To design a suitable $B^k$ we shall consider the quadratic approximation of $f$:\n",
        "\n",
        "\\begin{align}\n",
        "\\tilde{f}(\\mathbf{x}) = f(\\mathbf{x}^{k+1}) + \\left \\langle \\nabla f(\\mathbf{x}^{k+1}), \\mathbf{x}-\\mathbf{x}^{k+1}\\right \\rangle  + \\frac{1}{2} (\\mathbf{x}-\\mathbf{x}^{k+1})^\\top H^{k+1} (\\mathbf{x}-\\mathbf{x}^{k+1}). \n",
        "\\end{align}\n",
        "where $H^{k+1} = \\nabla^2 f({\\mathbf{x}}^{k+1})$.\n",
        "\n",
        "Note that using this quadratic approximation we have the gradient as:\n",
        "\\begin{align}\n",
        "\\nabla \\tilde{f}(\\mathbf{x}) = \\nabla f(\\mathbf{x}^{k+1}) + H^{k+1}(\\mathbf{x}-\\mathbf{x}^{k+1}). \n",
        "\\end{align}\n",
        "\n",
        "In order to assume $\\tilde{f}$ to behave similar to $f$, we expect the following. \n",
        "\n",
        "By plugging in $\\mathbf{x} = \\mathbf{x}^k$ and $\\mathbf{x}=\\mathbf{x}^{k+1}$, we expect the following from the previous gradient equation:\n",
        "\\begin{align}\n",
        "\\nabla \\tilde{f} (\\mathbf{x}^k) = \\nabla f(\\mathbf{x}^k) \\text{ and }\\\\ \n",
        "\\nabla \\tilde{f} (\\mathbf{x}^{k+1}) = \\nabla f(\\mathbf{x}^{k+1}). \n",
        "\\end{align}\n",
        "\n",
        "The relation $\\nabla \\tilde{f} (\\mathbf{x}^{k+1}) = \\nabla f(\\mathbf{x}^{k+1})$ directly follows from the gradient relation  $\\nabla \\tilde{f}(\\mathbf{x}) = \\nabla f(\\mathbf{x}^{k+1}) + H^{k+1}(\\mathbf{x}-\\mathbf{x}^{k+1})$.\n",
        "\n",
        "For the gradient relation to satisfy $\\nabla \\tilde{f} (\\mathbf{x}^k) = \\nabla f(\\mathbf{x}^k)$ we need:\n",
        "\\begin{align}\n",
        "\\nabla \\tilde{f} (\\mathbf{x}^k) &= \\nabla f(\\mathbf{x}^{k+1}) + H^{k+1}(\\mathbf{x}^{k}-\\mathbf{x}^{k+1}) = \\nabla f(\\mathbf{x}^k) \\\\\n",
        "\\implies H^{k+1}(\\mathbf{x}^{k}-\\mathbf{x}^{k+1}) &= (\\nabla f(\\mathbf{x}^{k})- \\nabla {f} (\\mathbf{x}^{k+1})) \\\\\n",
        "\\implies H^{k+1}(\\mathbf{x}^{k+1}-\\mathbf{x}^{k}) &= (\\nabla f(\\mathbf{x}^{k+1})- \\nabla {f} (\\mathbf{x}^k)).\n",
        "\\end{align}\n",
        "This previous equality is called the $\\textbf{secant equation}$. \n",
        "\n",
        "From the secant equation we see that inverse of $H^{k+1}$ operates on the difference of gradients $(\\nabla f(\\mathbf{x}^{k+1})- \\nabla {f} (\\mathbf{x}^k))$  to yield the difference of iterates $(\\mathbf{x}^{k+1}-\\mathbf{x}^{k})$. \n",
        "\n",
        "The secant equation can be equivalently and compactly written as:\n",
        "\\begin{align}\n",
        "(H^{k+1})^{-1} \\mathbf{y}^k = \\mathbf{s}^k. \n",
        "\\end{align}\n",
        "where $\\mathbf{y}^k = (\\nabla f(\\mathbf{x}^{k+1})- \\nabla {f} (\\mathbf{x}^k))$ and $\\mathbf{s}^k = (\\mathbf{x}^{k+1}-\\mathbf{x}^{k})$. \n",
        "\n",
        "We shall be considering $(H^{k+1})^{-1}$ as a possible choice for $B^{k+1}$ in the BFGS update rule. \n",
        "\n",
        "Hence we make sure that $(H^{k+1})^{-1}$ is positive definite. This is equivalent to considering: \n",
        "\\begin{align}\n",
        "(\\mathbf{y}^{k})^\\top (H^{k+1})^{-1} \\mathbf{y}^k > 0 \n",
        "\\end{align}\n",
        "for any non-zero $\\mathbf{y}^k$ which implies that $(\\mathbf{y}^k)^\\top \\mathbf{s}^k > 0$. \n",
        "\n",
        "\n",
        "Generally solving the secant equation $(H^{k+1})^{-1} \\mathbf{y}^k = \\mathbf{s}^k$ leads to infinitely many solutions for the matrix $(H^{k+1})^{-1}$ since there are $n^2$ unknowns and $n$ equations. Hence to select a suitable $(H^{k+1})^{-1}$ we solve an optimization problem of the form: \n",
        "\n",
        "\\begin{align}\n",
        "\\min_H \\|H-(H^k)^{-1}\\| \\ s.t. \\ H=H^\\top, \\ H\\mathbf{y}^k=\\mathbf{s}^k.\n",
        "\\end{align}\n",
        "By using an appropriate norm in the optimization problem, we can get the following update rule for the matrix $(H^{k+1})^{-1} = (I-\\mu^k \\mathbf{s}^k (\\mathbf{y}^k)^\\top) (H^{k})^{-1} (I-\\mu^k \\mathbf{y}^k (\\mathbf{s}^k)^\\top) + \\mu^k \\mathbf{s}^k (\\mathbf{s}^k)^\\top$\n",
        "\n",
        "where $\\mu^k = \\frac{1}{(\\mathbf{y}^k)^\\top \\mathbf{s}^k}$.\n",
        "\n",
        "By taking $B^k = (H^k)^{-1}$, this update rule can now be written as:\n",
        "\n",
        "$B^{k+1} = (I-\\mu^k \\mathbf{s}^k (\\mathbf{y}^k)^\\top) B^{k} (I-\\mu^k \\mathbf{y}^k (\\mathbf{s}^k)^\\top) + \\mu^k \\mathbf{s}^k (\\mathbf{s}^k)^\\top$\n",
        "\n",
        "where $\\mu^k = \\frac{1}{(\\mathbf{y}^k)^\\top \\mathbf{s}^k}$.\n",
        "\n",
        "As long as $B^k$ is positive definite, the update rule guarantees that $B^{k+1}$ is also positive definite. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Meohokl4xP"
      },
      "source": [
        "Hence in Exercises 1 and 2, we shall be implementing BFGS method to solve problems of the form $\\min_{\\mathbf{x}\\in{\\mathbb{R}}^n} f(\\mathbf{x})$, and check its  performance against Newton method. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 1) We can choose $B^0$ to be a scalar multiple of an identity matrix as it is a symmetric positive definite matrix."
      ],
      "metadata": {
        "id": "_seXeSTqz0cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let us now check the time taken for computing the inverse of a matrix A\n",
        "from timeit import default_timer as timer\n",
        "import numpy as np \n",
        "\n",
        "#create a random nxn matrix \n",
        "n = 100\n",
        "B = np.random.rand(n, n)\n",
        "A = np.matmul(B,B.T) #Note: This construction ensures that A is symmetric\n",
        "A = np.add(A, 0.001*np.identity(n)) #this diagonal perturbation helps to make the matrix positive definite \n",
        "\n",
        "start_time = timer()\n",
        "A_inv = np.linalg.inv(A)\n",
        "end_time = timer()\n",
        "print('Time taken to compute inverse of A:',end_time - start_time) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIMl937EXeFK",
        "outputId": "7dab1d4e-50a5-4a8f-a140-05c284f85432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to compute inverse of A: 0.0009885139988909941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcsCIAntMNdb"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x, n):  \n",
        "    #Input: x is a numpy array of size n \n",
        "    assert type(x) is np.ndarray \n",
        "    assert len(x) == n \n",
        "    f = 0\n",
        "    for i in range(n-1):\n",
        "        f += (4*(x[i]**2-x[i+1])**2 + (x[i] - 1)**2)\n",
        "\n",
        "    return (f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA1zWRO0xMd6"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x, n):  \n",
        "    #Input: x is a numpy array of size n\n",
        "    assert type(x) is np.ndarray  \n",
        "    assert len(x) == n \n",
        "    gradient = np.zeros(n)\n",
        "    gradient[0] = 16*x[0]*(x[0]**2 - x[1]) + 2*(x[0]-1)\n",
        "    gradient[n-1] = 8*(x[n-1] - (x[n-2]**2))\n",
        "    for i in range(1,n-1):\n",
        "        gradient[i] = (-8*(x[i-1]**2 - x[i]) + 16*x[i]*(x[i]**2 - x[i+1]) + 2*(x[i]-1))\n",
        "    return gradient"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evalh(x,n):\n",
        "    assert type(x) is np.ndarray  #do not allow arbitrary type arguments \n",
        "    assert len(x) == n #do not allow arbitrary size arguments\n",
        "\n",
        "    hessian = np.zeros((n,n))\n",
        "    hessian[0][0] = 16*(3*x[0]**2 - x[1]) + 2\n",
        "    hessian[0][1] = -16*x[0]\n",
        "    \n",
        "    hessian[n-1][n-1] = 8\n",
        "    hessian[n-1][n-2] = -16*x[n-2]\n",
        "    \n",
        "    for i in range(1,n-1):\n",
        "        hessian[i][i] = 16*(3*(x[i]**2)-x[i+1]) + 10\n",
        "        hessian[i][i-1] = -16*x[i-1]\n",
        "        hessian[i][i+1] = -16*x[i] \n",
        "\n",
        "    return hessian"
      ],
      "metadata": {
        "id": "OAcpS-n_2LtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_steplength_backtracking(x, n, gradf, D_k, alpha_start, rho, gamma): #add appropriate arguments to the function \n",
        "    assert type(x) is np.ndarray and len(gradf) == n\n",
        "    assert type(gradf) is np.ndarray and len(gradf) == n \n",
        "    assert type(alpha_start) is float and alpha_start>=0. \n",
        "    assert type(rho) is float and rho>=0.\n",
        "    assert type(gamma) is float and gamma>=0. \n",
        "    assert type(D_k) is np.ndarray and len(D_k) == n\n",
        "    \n",
        "    alpha = alpha_start\n",
        "    pk = -gradf\n",
        "    while evalf(np.add(x, alpha*np.dot(D_k,pk)), n) > np.subtract(evalf(x, n), gamma*alpha*np.dot(np.dot(D_k,gradf), gradf)):\n",
        "    \n",
        "        alpha = rho*alpha\n",
        "    \n",
        "    return alpha"
      ],
      "metadata": {
        "id": "tybYuY9s2vdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to construct D_k matrix used in Newton's method\n",
        "def compute_D_k_newton(x, n):\n",
        "    assert type(x) is np.ndarray\n",
        "    assert len(x) == n\n",
        "    \n",
        "    return np.linalg.inv(evalh(x, n))"
      ],
      "metadata": {
        "id": "ShrkerC23xZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#line search type \n",
        "CONSTANT_STEP_LENGTH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2"
      ],
      "metadata": {
        "id": "ojnS9Wqx3z4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 2)"
      ],
      "metadata": {
        "id": "mO-kN_rc0N4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_BFGS(start_x, n, tol, line_search_type, *args):\n",
        "    assert type(start_x) is np.ndarray  \n",
        "    assert len(start_x) == n  \n",
        "    assert type(tol) is float and tol>=0 \n",
        "\n",
        "    x = start_x\n",
        "    g_x = evalg(x,n)\n",
        "    h_x = evalh(x,n)\n",
        "\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "        if args is None:\n",
        "            err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any arguments'\n",
        "            raise ValueError(err_msg)\n",
        "        elif len(args)<3 :\n",
        "            err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three arguments'\n",
        "            raise ValueError(err_msg)\n",
        "        else:\n",
        "            alpha_start = float(args[0])\n",
        "            rho = float(args[1])\n",
        "            gamma = float(args[2])\n",
        "    k = 0\n",
        "    B = np.identity(n)\n",
        "\n",
        "    while (np.linalg.norm(g_x) > tol): \n",
        "        d_k = B\n",
        "        p = -np.dot(B, g_x)\n",
        "        if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "            step_length = compute_steplength_backtracking(x, n, g_x, d_k, alpha_start, rho, gamma) \n",
        "        elif line_search_type == CONSTANT_STEP_LENGTH: \n",
        "            step_length = 1.0\n",
        "        else:  \n",
        "            raise ValueError('Line search type unknown')\n",
        "        \n",
        "        # Gradient descent steps\n",
        "        x_next = np.add(x, np.multiply(step_length,p)) \n",
        "        s_k = x_next - x\n",
        "        y_k = evalg(x_next,n) - g_x\n",
        "        mu_k = 1.0/np.dot(y_k.T, s_k)\n",
        "        temp = (np.identity(n) - np.multiply(mu_k,np.outer(y_k, s_k.T)))\n",
        "        B = np.dot(temp.T, np.dot(B, temp)) + np.multiply(mu_k, np.outer(s_k, s_k.T))\n",
        "        k += 1 \n",
        "        x = x_next\n",
        "        g_x = evalg(x,n)\n",
        "        # print('iter:',k, ' x:', x, ' f(x):', evalf(x,n), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "\n",
        "    return x, k, evalf(x,n)"
      ],
      "metadata": {
        "id": "olMNp0uy2z5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 4)"
      ],
      "metadata": {
        "id": "UtuKsYwt0Wv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_minimizer_Newtonmethod(start_x, n, tol, line_search_type, *args):\n",
        "    #Input: start_x is a numpy array of size n, tol denotes the tolerance and is a positive float value\n",
        "    assert type(start_x) is np.ndarray \n",
        "    assert len(start_x) == n \n",
        "    assert type(tol) is float and tol>=0 \n",
        "\n",
        "    x = start_x\n",
        "    g_x = evalg(x,n)\n",
        "    h_x = evalh(x,n)\n",
        "\n",
        "    if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "        if args is None:\n",
        "            err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive any arguments'\n",
        "            raise ValueError(err_msg)\n",
        "        elif len(args)<3 :\n",
        "            err_msg = 'Line search type: BACKTRACKING_LINE_SEARCH, but did not receive three arguments'\n",
        "            raise ValueError(err_msg)\n",
        "        else:\n",
        "            alpha_start = float(args[0])\n",
        "            rho = float(args[1])\n",
        "            gamma = float(args[2])\n",
        "    k = 0\n",
        "\n",
        "    while (np.linalg.norm(g_x) > tol):\n",
        "        d_k = compute_D_k_newton(x, n)\n",
        "        if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "            step_length = compute_steplength_backtracking(x, n, g_x, d_k, alpha_start, rho, gamma) \n",
        "        elif line_search_type == CONSTANT_STEP_LENGTH: \n",
        "            step_length = 1.0\n",
        "        else:  \n",
        "            raise ValueError('Line search type unknown')\n",
        "        \n",
        "        # Gradient descent steps\n",
        "        x = np.subtract(x, np.multiply(step_length,np.dot(d_k, g_x))) \n",
        "        k += 1 \n",
        "        g_x = evalg(x,n) \n",
        "\n",
        "    return x, k, evalf(x,n)"
      ],
      "metadata": {
        "id": "OwAfVJsA4DHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = [1000, 2500, 5000, 7500, 10000]\n",
        "my_tol= 1e-3\n",
        "alpha_start = 0.9\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ],
      "metadata": {
        "id": "0WWbEpzE9GWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 3)"
      ],
      "metadata": {
        "id": "ur2UrxDD0ZA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minimizer_bfgs = [0 for i in range(5)]\n",
        "iters_bfgs  = [0 for i in range(5)]\n",
        "min_fval_bfgs = [0 for i in range(5)]\n",
        "time_taken_bfgs = [0 for i in range(5)]\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    my_start_x = np.zeros(n[i])  \n",
        "    start_time = timer()\n",
        "    minimizer_bfgs[i] , iters_bfgs[i] , min_fval_bfgs[i] = find_minimizer_BFGS(my_start_x, n[i], my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho, gamma)\n",
        "    end_time = timer()\n",
        "    time_taken_bfgs[i] = end_time - start_time\n",
        "    print(\"n:\", n[i])\n",
        "    print(\"Minimum function value:\", min_fval_bfgs[i])\n",
        "    print(\"No of iterations:\", iters_bfgs[i])\n",
        "    print(\"Time taken:\", time_taken_bfgs[i])\n",
        "    print(\"Minimizer:\", minimizer_bfgs[i])\n",
        "    print(\"\\n***************************************************************************************************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL4_aEXe7vak",
        "outputId": "f20dfacd-8912-4f10-8e78-90432de21fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n: 1000\n",
            "Minimum function value: 1.509765174423235e-08\n",
            "No of iterations: 63\n",
            "Time taken: 42.113621098\n",
            "Minimizer: [0.99999852 0.99999906 0.9999996  0.9999995  0.99999973 0.99999922\n",
            " 0.99999952 0.9999991  0.99999961 0.99999917 0.9999997  0.99999908\n",
            " 0.99999979 0.99999914 0.99999967 0.99999946 0.99999961 0.99999945\n",
            " 0.99999955 0.99999931 0.99999955 0.99999935 0.99999957 0.99999932\n",
            " 0.99999961 0.9999992  0.99999951 0.9999991  0.9999995  0.99999901\n",
            " 0.99999955 0.99999917 0.99999956 0.99999931 0.99999983 0.99999962\n",
            " 0.99999978 0.99999982 0.99999934 0.99999965 0.99999905 0.99999996\n",
            " 0.9999986  0.99999947 0.99999939 0.9999995  0.99999963 0.99999989\n",
            " 0.99999907 0.99999939 0.99999942 0.9999995  0.99999965 0.99999877\n",
            " 1.00000031 0.9999991  0.99999916 0.99999986 0.99999922 0.99999951\n",
            " 0.99999944 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945 0.99999945\n",
            " 0.99999945 0.99999944 0.99999958 0.99999878 1.00000091 0.99999896\n",
            " 0.99999563 1.0000054  0.99999964 0.99999324 1.00000279 1.0000006\n",
            " 0.99999743 0.99999662 1.00000452 1.00000592 0.99999505 0.9999945\n",
            " 0.99999781 0.99999689 0.99999996 1.00000187 0.99999795 1.00000203\n",
            " 1.00000514 1.00000527 1.00000091 1.00000008 1.00000062 0.99999874\n",
            " 0.99999947 0.99999825 0.99999783 0.99999679 0.99999737 0.9999972\n",
            " 0.99999739 0.99999782 0.99999862 0.99999797 0.999998   0.99999837\n",
            " 0.99999877 0.99999942 1.00000018 1.00000029 0.99999995 1.00000006\n",
            " 0.99999813 1.0000005  0.99999999 0.99999874 0.99999948 0.99999885\n",
            " 0.99999882 0.99999927 1.00000146 1.00000193 0.99999995 0.99999485\n",
            " 0.99998856 0.99997952 0.99996218 0.99989054]\n",
            "\n",
            "***************************************************************************************************************************************\n",
            "n: 2500\n",
            "Minimum function value: 7.271484821663556e-09\n",
            "No of iterations: 67\n",
            "Time taken: 176.241088166\n",
            "Minimizer: [1.00000036 1.00000028 1.00000025 ... 1.00000513 1.00000925 1.00002114]\n",
            "\n",
            "***************************************************************************************************************************************\n",
            "n: 5000\n",
            "Minimum function value: 8.464918426169494e-09\n",
            "No of iterations: 85\n",
            "Time taken: 1572.0553763\n",
            "Minimizer: [0.99999869 0.99999853 0.99999952 ... 0.99999263 0.99998436 0.99996509]\n",
            "\n",
            "***************************************************************************************************************************************\n",
            "n: 7500\n",
            "Minimum function value: 7.487095768070494e-09\n",
            "No of iterations: 85\n",
            "Time taken: 5060.609278034\n",
            "Minimizer: [0.99999954 0.99999972 1.00000066 ... 0.99999708 0.99999346 0.99998351]\n",
            "\n",
            "***************************************************************************************************************************************\n",
            "n: 10000\n",
            "Minimum function value: 1.3295625380358626e-08\n",
            "No of iterations: 80\n",
            "Time taken: 11195.230018600003\n",
            "Minimizer: [1.00000046 1.0000006  1.00000115 ... 0.99998318 0.99995598 0.99989704]\n",
            "\n",
            "***************************************************************************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "time_taken_bfgs=[42.113621098, 176.241088166, 1572.0553763, 5060.609278034, 11195.230018600003]"
      ],
      "metadata": {
        "id": "tLeU-eMns8La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Storing time taken by BFGS method (in seconds) in a list so that the values dont get lost."
      ],
      "metadata": {
        "id": "gZXfQDD_04fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken by BFGS Method for each n\")\n",
        "print(\"n \\t Time taken\")\n",
        "for i in range(5):\n",
        "  print(n[i],\"\\t\",time_taken_bfgs[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-LUlGNeH7MG",
        "outputId": "71170ac4-3efb-4be0-d903-3b70950eb643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken by BFGS Method for each n\n",
            "n \t Time taken\n",
            "1000 \t 42.113621098\n",
            "2500 \t 176.241088166\n",
            "5000 \t 1572.0553763\n",
            "7500 \t 5060.609278034\n",
            "10000 \t 11195.230018600003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for answer 5)"
      ],
      "metadata": {
        "id": "COx9TLRO0b3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_tol= 1e-1"
      ],
      "metadata": {
        "id": "a08mTw-iA79I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minimizer_newton = [0 for i in range(5)]\n",
        "iters_newton  = [0 for i in range(5)]\n",
        "min_fval_newton = [0 for i in range(5)]\n",
        "time_taken_newton = [0 for i in range(5)]\n",
        "\n",
        "for i in range(5):\n",
        "    my_start_x = np.zeros(n[i])\n",
        "    start_time = timer()\n",
        "    minimizer_newton[i] , iters_newton[i] , min_fval_newton[i] = find_minimizer_Newtonmethod(my_start_x, n[i], my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho, gamma)\n",
        "    end_time = timer()\n",
        "    time_taken_newton[i] = end_time - start_time\n",
        "    print(\"n:\", n[i])\n",
        "    print(\"Minimum function value:\", min_fval_newton[i])\n",
        "    print(\"No of iterations:\", iters_newton[i])\n",
        "    print(\"Time taken:\", time_taken_newton[i])\n",
        "    print(\"Minimizer:\", minimizer_newton[i])\n",
        "    print(\"\\n***************************************************************************************************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "NTYhAzWl4u-7",
        "outputId": "113f5629-343d-4a2d-aeef-6a39c1237f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1a08128afa14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmy_start_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx_opts_newton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0miters_newton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mf_opts_newton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_minimizer_Newtonmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_start_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtimes_newton\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-9c1b2406db64>\u001b[0m in \u001b[0;36mfind_minimizer_Newtonmethod\u001b[0;34m(start_x, n, tol, line_search_type, *args)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0md_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_D_k_newton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBACKTRACKING_LINE_SEARCH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_steplength_backtracking_scaled_direction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mline_search_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCONSTANT_STEP_LENGTH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mstep_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9c38da334477>\u001b[0m in \u001b[0;36mcompute_steplength_backtracking_scaled_direction\u001b[0;34m(x, n, gradf, D_k, alpha_start, rho, gamma)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The code was running for more than 4 hours even after taking a tolerance of $10^{-1}$ but it did not terminate for any value of n so I interrupted execution. This might be because computing inverse of a large Hessian matrix takes a lot of time and here our n (size of matrix) is very large. Also, step length is zero so at each iteration, the value of the objective function is the same, so Newton's method is not a suitable algorithm to find the minimum of this function. Hence, I did not run the next cell also."
      ],
      "metadata": {
        "id": "qZV0wNRb1E4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Time taken by Newton's method for each n\")\n",
        "print(\"n \\t Time taken\")\n",
        "for i in range(5):\n",
        "  print(n[i],\"\\t\",time_taken_newton[i])"
      ],
      "metadata": {
        "id": "n5JcsWfPIebr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 6)"
      ],
      "metadata": {
        "id": "B9t6B1s70giL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I have only plotted times taken by BFGS method and not both because of reasons mentioned above."
      ],
      "metadata": {
        "id": "4j-EFCX7163_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we will plot the function values and check the behavior\n",
        "import matplotlib.pyplot as plt #package useful for plotting\n",
        "plt.plot(n, time_taken_bfgs, '-o', label='BFGS', color='green')\n",
        "#plt.plot(n, time_taken_newton, '-o', label=\"Newton's Method\")\n",
        "plt.xlabel('n')\n",
        "plt.ylabel('Time taken')\n",
        "plt.legend()\n",
        "plt.xticks(n)\n",
        "plt.title(\"Time taken by BFGS method vs n\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "h6yFHyY-_cM8",
        "outputId": "06c5d19e-5cc1-416f-9ac3-411244c9bfa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "*   Time taken by the BFGS method to find minimizer increases with increase in n.\n",
        "*   The minimum function value is close to zero and the minimizer is close to $[1,1,...,1] \\in R^n$ for all values of n.\n",
        "\n"
      ],
      "metadata": {
        "id": "0er8GcRD2M5K"
      }
    }
  ]
}