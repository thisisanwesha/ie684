{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 1 Exercise 2. }$\n",
        "\n",
        "Now we will consider a slightly different algorithm which can be used to find a minimizer of the function $f(\\mathbf{x})=f(x_1,x_2)= (x_1+100)^2 + (x_2-25)^2$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gpe6eGRLvSh"
      },
      "source": [
        "$\\textbf{[R]}$ Write the function $f(\\mathbf{x})$ in the form $\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{x}\\in {\\mathbb{R}}^2$, $\\mathbf{A}$ is a symmetric matrix of size $2 \\times 2$, $\\mathbf{b}\\in{\\mathbb{R}}^2$ and $c\\in\\mathbb{R}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPeLBt0L7F7"
      },
      "source": [
        "Answer 1)\n",
        "\n",
        "$\\begin{bmatrix}\n",
        "x_1 & x_2\\\\\n",
        "\\end{bmatrix}\n",
        "*\n",
        "\\begin{bmatrix}\n",
        "1 & 0\\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}\n",
        "*\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\n",
        "\\end{bmatrix}\n",
        "+2*\n",
        "\\begin{bmatrix}\n",
        "100 & -25\n",
        "\\end{bmatrix}\n",
        "*\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\n",
        "\\end{bmatrix}\n",
        "+10625$\n",
        "\n",
        "\n",
        "A=$\\begin{bmatrix}\n",
        "1 & 0\\\\\n",
        "0 & 1\n",
        "\\end{bmatrix}$\n",
        "\n",
        "b=$\\begin{bmatrix}\n",
        "100 \\\\\n",
        "-25\n",
        "\\end{bmatrix}$\n",
        "\n",
        "c=10625"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjANnIQ3L39D"
      },
      "source": [
        "\n",
        "$\\textbf{[R]}$ It turns out that for a function $f:{\\mathbb{R}}^n\\rightarrow \\mathbb{R}$ of the form $f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$, where $\\mathbf{A}\\in{\\mathbb{R}}^{n \\times n}$ is a symmetric matrix, $\\mathbf{b} \\in {\\mathbb{R}}^n$ and $c\\in \\mathbb{R}$, the analytical solution to $\\min_{\\alpha \\geq 0} f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ can be found in closed form. Find the solution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jU-adJ0L-P1"
      },
      "source": [
        "Answer 2)\n",
        "\n",
        "$f(\\mathbf{x})=\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} + 2 \\mathbf{b}^\\top \\mathbf{x} + c$\n",
        "\n",
        "where $\\mathbf{A}$ is a symmetric matrix.\n",
        "\n",
        "The gradient is given by $ \\nabla f(\\mathbf{x})=2*\\mathbf{A}*\\mathbf{x} + 2 * \\mathbf{b}$\n",
        "\n",
        "We have to compute step length $\\alpha$ that minimizes the function $f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$ where $\\alpha \\ge 0$\n",
        "\n",
        "Let g($\\alpha$)=$f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))$\n",
        "\n",
        "Necessary condition for optimality is $g'(\\alpha)=0$\n",
        "\n",
        "Taking derivative with respect to $\\alpha$\n",
        "\n",
        "$\\frac{d\\ g(\\alpha)}{d \\ \\alpha}$=$\\frac{d(f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))}{d(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))}*\\frac{d(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))}{d\\alpha}=0$\n",
        "\n",
        "By chain rule \n",
        "\n",
        "$\\Rightarrow (\\nabla f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})))^\\top*\\nabla f(\\mathbf{x})=0 $\n",
        "\n",
        "Taking transpose both side \n",
        "\n",
        "$\\Rightarrow (\\nabla f(\\mathbf{x}))^\\top*(\\nabla f(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})))=0 $\n",
        "\n",
        "since $  \\nabla f(\\mathbf{x})=2*\\mathbf{A}*\\mathbf{x} + 2 * \\mathbf{b}$ \n",
        "\n",
        "$\\Rightarrow (\\nabla f(\\mathbf{x}))^\\top*[2*A*(\\mathbf{x} - \\alpha \\nabla f(\\mathbf{x}))+2*b]=0 $\n",
        "\n",
        "$\\Rightarrow (\\nabla f(\\mathbf{x}))^\\top*A*\\mathbf{x} -\\alpha * (\\nabla f(\\mathbf{x}))^\\top * A * (\\nabla f(\\mathbf{x})) + (\\nabla f(\\mathbf{x}))^\\top*\\mathbf{b}=0$\n",
        "\n",
        "$\\Rightarrow \\alpha * (\\nabla f(\\mathbf{x}))^\\top * A * (\\nabla f(\\mathbf{x}))=(\\nabla f(\\mathbf{x}))^\\top * A* \\mathbf{x}+(\\nabla f(\\mathbf{x}))^\\top * \\mathbf{b}$\n",
        "\n",
        "$\\Rightarrow \\alpha=\\frac{(\\nabla f(\\mathbf{x}))^\\top * A* \\mathbf{x}+(\\nabla f(\\mathbf{x}))^\\top * \\mathbf{b}}{(\\nabla f(\\mathbf{x}))^\\top * A * (\\nabla f(\\mathbf{x}))}$\n",
        "\n",
        "$\\Rightarrow \\alpha=\\frac{(\\nabla f(\\mathbf{x}))^\\top * (A* \\mathbf{x}+\\mathbf{b})}{(\\nabla f(\\mathbf{x}))^\\top * A * (\\nabla f(\\mathbf{x}))}$\n",
        "\n",
        "$\\Rightarrow \\alpha=\\frac{(\\nabla f(\\mathbf{x}))^\\top \\nabla f(\\mathbf{x})}{2(\\nabla f(\\mathbf{x}))^\\top * A * (\\nabla f(\\mathbf{x}))}$\n",
        "\n",
        "where $\\nabla f(\\mathbf{x})=2*\\mathbf{A}*\\mathbf{x} + 2 * \\mathbf{b}$\n",
        "\n",
        "Now we have to prove that $\\alpha $ is minimizer by showing\n",
        "\n",
        "$\\frac{d^2 (g(\\alpha))}{d\\alpha^2}>0 $\n",
        "\n",
        "$\\nabla^2f(\\mathbf(x))=2*A $ \n",
        "\n",
        "which is symmetric and positive definite\n",
        "\n",
        "$\\Rightarrow \\alpha$ is minimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVkab74DJsRL"
      },
      "source": [
        "We will use this idea to construct a suitable step length finding procedure for our modified algorithm given below: \n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "& \\textbf{Input:} \\text{ Starting point $x^0$, Stopping tolerance $\\tau$}  \\\\\n",
        "& \\textbf{Initialize } k=0 \\\\ \n",
        "&\\textbf{While } \\| \\nabla f(\\mathbf{x}^k) \\|_2 > \\tau \\text{ do:}  \\\\   \n",
        "&\\quad \\quad \\eta^k = \\arg\\min_{\\eta\\geq 0} f(\\mathbf{x}^k - \\eta  \\nabla f(\\mathbf{x}^k)) \\\\\n",
        "&\\quad \\quad \\mathbf{x}^{k+1} \\leftarrow \\mathbf{x}^k - \\eta^k \\nabla f(\\mathbf{x}^k)  \\\\ \n",
        "&\\quad \\quad k = {k+1} \\\\ \n",
        "&\\textbf{End While} \\\\\n",
        "&\\textbf{Output: } \\mathbf{x}^k\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "source": [
        "#numpy package will be used for most of our lab exercises. Please have a look at https://numpy.org/doc/stable/ for numpy documentation\n",
        "#we will first import the numpy package and name it as np\n",
        "import numpy as np \n",
        "#Henceforth, we can lazily use np to denote the much longer numpy !! "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "source": [
        "#Now we will define a function which will compute and return the function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  return (x[0]+100)**2 + (x[1]-25)**2\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "source": [
        "#Now we will define a function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  return np.array([2*(x[0]+100),2*(x[1]-25)])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3blM08V0HOl"
      },
      "source": [
        "#Complete the module to compute the steplength\n",
        "def compute_steplength(x): #add appropriate arguments to the function \n",
        "  #Complete the code\n",
        "  A=np.array([[1,0],[0,1]])\n",
        "  num=np.dot(evalg(x),evalg(x))\n",
        "  deno=np.dot(np.dot(evalg(x),A),evalg(x))\n",
        "  step_length=num/(2*deno)\n",
        "  return step_length"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "source": [
        "def find_minimizer(start_x, tol):\n",
        "  #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "  assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "  assert type(tol) is float and tol>=0 \n",
        "  x = start_x\n",
        "  g_x = evalg(x)\n",
        "  k = 0\n",
        "  print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "  while (np.linalg.norm(g_x) > tol): #continue as long as the norm of gradient is not close to zero upto a tolerance tol\n",
        "    step_length = compute_steplength(x) #call the new function you wrote to compute the steplength\n",
        "    x = np.subtract(x, np.multiply(step_length,g_x)) #update x = x - step_length*g_x\n",
        "    k += 1 #increment iteration\n",
        "    g_x = evalg(x) #compute gradient at new point\n",
        "    print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "  return k\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-kHCkbwe-M4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49264590-5d5e-457b-8299-458bf6548323"
      },
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= 1e-3\n",
        "find_minimizer(my_start_x, my_tol)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_start_x = np.array([10,10])\n",
        "my_tol= [10**(-(p+1)) for p in range(10)]\n",
        "iters=[]\n",
        "for i in my_tol:\n",
        "  iters.append(find_minimizer(my_start_x,i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQhI2zDdOU6H",
        "outputId": "6308a2e0-9e2c-4267-8c8b-a1ad34feef4a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n",
            "iter: 0  x: [10 10]  f(x): 12325  grad at x: [220 -30]  gradient norm: 222.03603311174518\n",
            "iter: 1  x: [-100.   25.]  f(x): 0.0  grad at x: [0. 0.]  gradient norm: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(my_tol,iters)\n",
        "plt.xlabel('Stopping tolerance values')\n",
        "plt.ylabel('Number of iterations')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "PZHEa8e7OXCX",
        "outputId": "f144104b-aa4d-4b5a-aa42-0d498deaaa75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 4) In exercise 1, as stopping tolerance values decreased, maximum number of iterations taken by the algorithm to terminate increased. Here, algorithm terminates after one iteration even if we change the stopping tolerance values."
      ],
      "metadata": {
        "id": "TievIqYGWPP4"
      }
    }
  ]
}