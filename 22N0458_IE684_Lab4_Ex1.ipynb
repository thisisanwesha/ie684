{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVE0Xoa0Q5wE"
      },
      "source": [
        "$\\Large\\textbf{Lab 4.}$ $\\large\\textbf{Exercise 1.}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "72qwiJ0CDtOX"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from numpy.linalg import cond\n",
        "from scipy.linalg import sqrtm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GZjX2IwOR8_X"
      },
      "outputs": [],
      "source": [
        "#Now we will define a Python function which will compute and return the objective function value \n",
        "def evalf(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the objective function value\n",
        "  #compute the function value and return it \n",
        "  return 400*x[0]**2 + 0.004*x[1]**4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6klpwtDra_I8"
      },
      "outputs": [],
      "source": [
        "#Now we will define a Python function which will compute and return the gradient value as a numpy array \n",
        "def evalg(x):  \n",
        "  #Input: x is a numpy array of size 2 \n",
        "  assert type(x) is np.ndarray and len(x) == 2 #do not allow arbitrary arguments \n",
        "  #after checking if the argument is valid, we can compute the gradient value\n",
        "  #compute the gradient value and return it \n",
        "  return np.array([800*x[0],0.016*x[1]**3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#method to find Hessian matrix: Complete the code\n",
        "def evalh(x): \n",
        "  assert type(x) is np.ndarray \n",
        "  assert len(x) == 2\n",
        "  return np.array([[800,0],[0,0.048*x[1]**2]])"
      ],
      "metadata": {
        "id": "CfbgcU_fhGV0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KJq7tIgIRroP"
      },
      "outputs": [],
      "source": [
        "#The method defines a way to construct D_k matrix used in scaling the gradient in the modified gradient descent scheme\n",
        "def compute_D_k(x):\n",
        "  assert type(x) is np.ndarray\n",
        "  assert len(x) == 2\n",
        "  #compute and return D_k\n",
        "  a=1/(evalh(x)[0][0])\n",
        "  b=1/(evalh(x)[1][1])\n",
        "  return np.array([[a, 0], [0, b]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fO57EUXi44n6"
      },
      "outputs": [],
      "source": [
        "def compute_D_k_newton(x):\n",
        "    assert type(x) is np.ndarray\n",
        "    assert len(x) == 2\n",
        "    \n",
        "    return np.linalg.inv(evalh(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PGunDYy6Q21S"
      },
      "outputs": [],
      "source": [
        "#Complete the module to compute the steplength by using the closed-form expression\n",
        "def compute_steplength_backtracking(x, gradf, alpha_start, rho, gamma, *args): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  #Complete the code \n",
        "  alpha = alpha_start\n",
        "  pk = -gradf\n",
        "  while evalf(np.add(x, alpha*pk)) > np.add(evalf(x), gamma*alpha*np.dot(gradf.T,pk)):\n",
        "    alpha = rho*alpha\n",
        "\n",
        "  return alpha\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wqjdHM3eaHYf"
      },
      "outputs": [],
      "source": [
        "def compute_steplength_backtracking_scaled_direction(x, gradf, direction, alpha_start, rho, gamma, *args): #add appropriate arguments to the function \n",
        "  assert type(x) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(gradf) is np.ndarray and len(gradf) == 2 \n",
        "  assert type(direction) is np.ndarray and len(direction) == 2 \n",
        "  assert type(alpha_start) is float and alpha_start>=0. \n",
        "  assert type(rho) is float and rho>=0.\n",
        "  assert type(gamma) is float and gamma>=0. \n",
        "  \n",
        "  \n",
        "  \n",
        "  #Complete the code \n",
        "  alpha = alpha_start\n",
        "  pk = -gradf\n",
        "  while evalf(np.add(x, alpha*np.dot(direction,pk))) > np.subtract(evalf(x), gamma*alpha*np.dot(np.dot(direction,gradf), gradf)):\n",
        "    alpha = rho*alpha\n",
        "  return alpha"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#line search type \n",
        "CONSTANT_STEP_LENGTH = 1\n",
        "BACKTRACKING_LINE_SEARCH = 2\n",
        "\n",
        "# D_k choice type\n",
        "newton = 3\n",
        "diagonal_approx = 4"
      ],
      "metadata": {
        "id": "6eqNQjDQ5lF7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kdYW5nldqZU-"
      },
      "outputs": [],
      "source": [
        "# Code for gradient descent with scaling to find the minimizer without scaling\n",
        "\n",
        "def find_minimizer_gd(start_x, tol, line_search_type, *args):\n",
        "    #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "    assert type(start_x) is np.ndarray and len(start_x) == 2 #do not allow arbitrary arguments \n",
        "    assert type(tol) is float and tol>=0 \n",
        "\n",
        "    x = start_x\n",
        "    g_x = evalg(x)\n",
        "\n",
        "    #initialization for backtracking line search\n",
        "    if(line_search_type == BACKTRACKING_LINE_SEARCH):\n",
        "        alpha_start = args[0]\n",
        "        rho = args[1]\n",
        "        gamma = args[2]\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    while (np.linalg.norm(g_x) > tol): \n",
        "    \n",
        "        if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "            step_length = compute_steplength_backtracking(x,g_x, alpha_start,rho, gamma) \n",
        "        elif line_search_type == CONSTANT_STEP_LENGTH: \n",
        "            step_length = 1.0\n",
        "        else:  \n",
        "            raise ValueError('Line search type unknown. Please check!')\n",
        "        \n",
        "        # Gradient descent steps   \n",
        "        x = np.subtract(x, np.multiply(step_length,g_x))\n",
        "        k += 1 \n",
        "        g_x = evalg(x) \n",
        "\n",
        "       # print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "    #print(k)\n",
        "    return x, evalf(x), k\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3SCJdqivdpxx"
      },
      "outputs": [],
      "source": [
        "# Code for gradient descent with scaling to find the minimizer with scaling\n",
        "\n",
        "def find_minimizer_gdscaling(start_x, tol, line_search_type, D_k_type, *args):\n",
        "    #Input: start_x is a numpy array of size 2, tol denotes the tolerance and is a positive float value\n",
        "    assert type(start_x) is np.ndarray and len(start_x) == 2 \n",
        "    assert type(tol) is float and tol>=0 \n",
        "\n",
        "    x = start_x\n",
        "    g_x = evalg(x)\n",
        "    \n",
        "    #initialization for backtracking line search\n",
        "    alpha_start = args[0]\n",
        "    rho = args[1]\n",
        "    gamma = args[2]\n",
        "\n",
        "    k = 0\n",
        "\n",
        "    while (np.linalg.norm(g_x) > tol): \n",
        "        if D_k_type == newton:\n",
        "            d_k = compute_D_k_newton(x)\n",
        "        elif D_k_type == diagonal_approx:\n",
        "            d_k = compute_D_k(x)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid argument.\")\n",
        "\n",
        "\n",
        "        if line_search_type == BACKTRACKING_LINE_SEARCH:\n",
        "            step_length = compute_steplength_backtracking_scaled_direction(x, g_x, d_k, alpha_start, rho, gamma) \n",
        "        elif line_search_type == CONSTANT_STEP_LENGTH: \n",
        "            step_length = 1.0\n",
        "        else:  \n",
        "            raise ValueError('Line search type unknown. Please check!')\n",
        "        \n",
        "        # Gradient descent steps\n",
        "        x = np.subtract(x, np.multiply(step_length,np.dot(d_k, g_x))) \n",
        "        k += 1 \n",
        "        g_x = evalg(x) \n",
        "        #print('iter:',k, ' x:', x, ' f(x):', evalf(x), ' grad at x:', g_x, ' gradient norm:', np.linalg.norm(g_x))\n",
        "\n",
        "\n",
        "    return x, evalf(x), k\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmoUV0J7_mH"
      },
      "source": [
        "Code for answer 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3hoak3jnHkXd"
      },
      "outputs": [],
      "source": [
        "my_start_x = np.array([2.,2.])\n",
        "my_tol= 1e-9\n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD2gfCVQ8szx",
        "outputId": "5d8d3e1f-e1c8-4cb1-8fff-410553da35ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [0.         0.00304488]\n",
            "Minimum function value = 3.4382653805813626e-13\n",
            "No of iterations = 16\n"
          ]
        }
      ],
      "source": [
        "#check gradient descent with backtracking line search \n",
        "x_opt_bls, f_bls, iter_bls = find_minimizer_gdscaling(my_start_x, my_tol, CONSTANT_STEP_LENGTH, newton, alpha_start, rho, gamma)\n",
        "print(\"Minimizer =\", x_opt_bls)\n",
        "print(\"Minimum function value =\", f_bls)\n",
        "print(\"No of iterations =\", iter_bls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRjcs5G29X9x",
        "outputId": "641b30be-7c4f-45a7-d335-adee6fc7a88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [0.         0.00304488]\n",
            "Minimum function value = 3.4382653805813626e-13\n",
            "No of iterations = 16\n"
          ]
        }
      ],
      "source": [
        "#check gradient descent with scaling and backtracking line search \n",
        "x_opt_bls_scaling, f_bls_scaling, iter_bls_scaling = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, newton, alpha_start, rho, gamma)\n",
        "print(\"Minimizer =\", x_opt_bls_scaling)\n",
        "print(\"Minimum function value =\", f_bls_scaling)\n",
        "print(\"No of iterations =\", iter_bls_scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u0JgT6Y8Rdm"
      },
      "source": [
        "Code for answer 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ubaEa5vF8x_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee2cb53-0f60-4e3d-bf43-4a7f159574f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [0.         0.00304488]\n",
            "Minimum function value = 3.4382653805813626e-13\n",
            "No of iterations = 16\n"
          ]
        }
      ],
      "source": [
        "#check gradient descent with scaling and backtracking line search \n",
        "x_opt_bls_scaling, f_bls_scaling, iter_bls_scaling = find_minimizer_gdscaling(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, diagonal_approx, alpha_start, rho, gamma)\n",
        "print(\"Minimizer =\", x_opt_bls_scaling)\n",
        "print(\"Minimum function value =\", f_bls_scaling)\n",
        "print(\"No of iterations =\", iter_bls_scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FxQvByQI9Uqi"
      },
      "outputs": [],
      "source": [
        "my_start_x = np.array([2.,2.])\n",
        "my_tol= 1e-6\n",
        "alpha_start = 1.\n",
        "rho = 0.5\n",
        "gamma = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tKSmaPCj9WEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5154e3f4-d9ee-4bc3-d4ef-dbad2a340850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer = [3.53148711e-10 3.91377499e-02]\n",
            "Minimum function value = 9.385197325158433e-09\n",
            "No of iterations = 5707240\n"
          ]
        }
      ],
      "source": [
        "#check gradient descent with backtracking line search \n",
        "x_opt_bls, f_bls, iter_bls = find_minimizer_gd(my_start_x, my_tol, BACKTRACKING_LINE_SEARCH, alpha_start, rho, gamma)\n",
        "print(\"Minimizer =\", x_opt_bls)\n",
        "print(\"Minimum function value =\", f_bls)\n",
        "print(\"No of iterations =\", iter_bls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pUL8EPQ2wTB"
      },
      "source": [
        "Answer 3) \n",
        "\n",
        "**Newton's method (with constant step length of 1)**\n",
        "\n",
        "Minimizer = [0.0 , 0.00304488]\n",
        "\n",
        "Minimum function value = 3.4382653805813626e-13\n",
        "\n",
        "No of iterations = 16\n",
        "\n",
        "**Newton's method (with Backtracking line search)**\n",
        "\n",
        "Minimizer = [0.0 , 0.00304488]\n",
        "\n",
        "Minimum function value = 3.4382653805813626e-13\n",
        "\n",
        "No of iterations = 16\n",
        "\n",
        "**Observations:** \n",
        "\n",
        "In both the cases, the minimizer is [0.0 , 0.00304488], the minimum function value is close to 0 (3.4382653805813626e-13) and the number of iterations is 16. We may say that the choice of the step-length did not affect the rate of convergence of the algorithm, rather the consideration of the scaling algorithm (with the scaling matrix taken as ($\\nabla^2 f(\\mathbf{x}))^{-1}$ in Newton's method) was more crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 4) \n",
        "\n",
        "**Gradient Descent (with scaling using diagonal matrix)**\n",
        "\n",
        "Minimizer = [0.0 , 0.00304488]\n",
        "\n",
        "Minimum function value = 3.4382653805813626e-13\n",
        "\n",
        "No of iterations = 16\n",
        "\n",
        "**Gradient Descent (without scaling)** \n",
        "\n",
        "(Here, tolerance was taken to be $10^{-6}$ because it was taking a long time to converge)\n",
        "\n",
        "Minimizer = [3.53148711e-10 3.91377499e-02]\n",
        "\n",
        "Minimum function value = 9.385197325158433e-09\n",
        "\n",
        "No of iterations = 5707240\n",
        "\n",
        "**Observations:** \n",
        "\n",
        "Graident Descent with scaling using diagonal matrix uses very less iterations (16) when compared to gradient descent without scaling (5707240 iterations) even though the tolerance levels in both are $10^{-9}$ and $10^{-6}$ respectively. Gradient Descent with scaling using diagonal matrix gave a better result (Minimizer close to (0,0) and minimum function value close to 0) than gradient descent without scaling.\n"
      ],
      "metadata": {
        "id": "OiX2jsLR-073"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}